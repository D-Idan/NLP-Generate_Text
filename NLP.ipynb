{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Neural Networks\n",
    "\n",
    "In this notebook we will create a network that can generate text, here we show it being done character by character. Very awesome write up on this here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZsDPKDUoup3q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pznJly24up3v"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKGnPVO-up3w"
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n7-d4A_Cup3x"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "215RZfiKup3y"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, mode='r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3tRwdZawup3y"
   },
   "outputs": [],
   "source": [
    "#print(text[:1000]) # repeated stracture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n7wldL0hup3y"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text)) # Unique Charecters in the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfZD1dtcup3z",
    "outputId": "10d32557-5d7f-4c6c-cd92-de88813e4d56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZfgfTpUup30"
   },
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "avY3wiXwup31"
   },
   "outputs": [],
   "source": [
    "####### enumerate(vocab)  =  count the unique charecters  ( enumerate them)\n",
    "#for pair in enumerate(vocab):\n",
    "#    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "128z9tqAup32"
   },
   "outputs": [],
   "source": [
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-yTLs-Zup32",
    "outputId": "c10ba697-0e97-41f9-c18d-0f5c32054f2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "827DwPxfup32"
   },
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bXbnmC7wup33",
    "outputId": "3131d19f-48f4-4a8c-ae44-6c3bc7de7c14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TseBECG5up33"
   },
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ZhQewPup33",
    "outputId": "dfd062da-5a8e-4aa0-80bc-aff701055fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mo6b-Dunup33",
    "outputId": "1dab7897-2e90-4111-e7f5-a5a81a7e4cf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fYahEdDAup34"
   },
   "outputs": [],
   "source": [
    "sample = text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58iQqkABup34",
    "outputId": "f1f18403-0ef8-43ae-a225-00be9fbfb2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou t\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTHkJeNHup34",
    "outputId": "9ab1cb36-975c-4aa0-d0d3-3f2b31824471"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjwypMHfup35"
   },
   "source": [
    "## Creating Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDFfWVxvup35"
   },
   "source": [
    "We would like to have batches that consist of the basic stracture of the text.\n",
    "In this text we have rhyme at almost every second line.\n",
    "we can choose to grab 3 lines for one rhyme or 4 for two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CaPfVQTZup35"
   },
   "outputs": [],
   "source": [
    "line = '''From fairest creatures we desire increase,'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PM8ePyyHup35",
    "outputId": "bd1fc96e-d166-4645-cc53-0e2ed9b0c750"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nnzc36Asup36"
   },
   "outputs": [],
   "source": [
    "lines = '''  From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\n",
    "  His tender heir might bear his memory:\n",
    "  But thou contracted to thine own bright eyes,\n",
    "  Feed'st thy light's flame with self-substantial fuel,\n",
    "  Making a famine where abundance lies,\n",
    "  Thy self thy foe, to thy sweet self too cruel:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yucC1Yjpup36",
    "outputId": "f2a0db9f-12ca-4f04-f7aa-d06d26e8bca6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn5Qzpcmup36",
    "outputId": "22cd6498-ee53-48c0-fe36-526b31b38bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "seq_len = 45*4 # We will choose batch of 4 lines\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lj_6qxeSup36"
   },
   "outputs": [],
   "source": [
    "total_num_seq = len(text) // (seq_len+1) # +1 For zero indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbPxidXNup36",
    "outputId": "c1cc896a-eb33-4703-d407-de4efba2cec1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30086"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM2xGCX9up37"
   },
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ec6_vIOrup37"
   },
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvYlju7Jup37"
   },
   "source": [
    "We where able to do the incoding on the tf.data.dataset object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jboNG64Eup37"
   },
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3vwhGfqmup37"
   },
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bUrmWID2up37"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUbwPYlFup38",
    "outputId": "a5ea2953-7d2e-4e9a-b6cd-6e455901a355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75\n",
      "  1 56 74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1\n",
      " 75 64 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59\n",
      " 60 73  1 63 60 64 73  1 68 64 62 63]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir migh\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1\n",
      " 56 74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1 75\n",
      " 64 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59 60\n",
      " 73  1 63 60 64 73  1 68 64 62 63 75]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might\n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(ind_to_char[input_txt.numpy()]))\n",
    "    print(\"\\n\")\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQ_6ZxLNup38",
    "outputId": "ff10ae42-a1b2-4407-e3dd-772ef6dd2482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2**7\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RAHQehRlup38"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzIv8uSXup38",
    "outputId": "79b22293-aa98-4978-f1e0-5c85d12a27cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 180), (128, 180)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANRCxcaKup38"
   },
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WoUfHyPup38",
    "outputId": "03a541b3-e6c3-4366-f0bd-08bcdcd3ac22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HLpwq0ePup39"
   },
   "outputs": [],
   "source": [
    "embd_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ne8PcrBNup39"
   },
   "outputs": [],
   "source": [
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "E4-GNETjup39"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pywDjYt4up39"
   },
   "source": [
    "https://keras.io/api/losses/probabilistic_losses/#sparse_categorical_crossentropy-function\n",
    "SparseCategoricalCrossentropy class\n",
    "\n",
    "tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"auto\", name=\"sparse_categorical_crossentropy\"\n",
    ")\n",
    "Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true.\n",
    "\n",
    "from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZgPZuSvjup3-"
   },
   "outputs": [],
   "source": [
    "#help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5KvBMNF_up3-"
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "oD8YAD4Mup3-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYNzsDi7up3-"
   },
   "source": [
    "Embedding Layer input size error:\n",
    "https://stackoverflow.com/questions/54557468/in-tf-keras-layers-embedding-why-it-is-important-to-know-the-size-of-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TxFO6tkbup3-"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size,embd_dim,rnn_neurons,batch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,  embd_dim, batch_input_shape = (batch_size,None)))\n",
    "    model.add(GRU(rnn_neurons, stateful=True,return_sequences=True,\n",
    "                  recurrent_initializer='glorot_uniform'))\n",
    "    # stateful=True -> use model-reset-states after eatch prediction\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "q9ftwVfdup3-"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size,embd_dim,rnn_neurons,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoVOEGVAup3_",
    "outputId": "9e61116f-007b-4cdf-ef87-5bd064e18462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2nyqLOdup3_"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWgZvPuJup3_",
    "outputId": "8c8d4640-611c-4a62-ea49-10feeee25e73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, None]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[batch_size,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FOeRt_Rv984"
   },
   "source": [
    "Before running the model, we will check it over one batch example to see how it works and if there are any issues to treat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "31uHkMQXup3_"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  example_batch_predictions = model(input_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqgDJoy3vdey",
    "outputId": "78db90e2-007a-4c5e-e72a-778e854bf228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 180, 84])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTe55geRvi26",
    "outputId": "fe8a0395-73a6-44eb-f051-a02dfbcb1365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(180, 84), dtype=float32, numpy=\n",
       "array([[-0.00313731,  0.00130075,  0.00269769, ..., -0.00196088,\n",
       "        -0.00358368,  0.00117394],\n",
       "       [ 0.00146231,  0.00368364,  0.00237321, ..., -0.00033735,\n",
       "         0.00222059, -0.0004087 ],\n",
       "       [-0.00168441,  0.00321426,  0.00164211, ..., -0.00426133,\n",
       "        -0.00303025, -0.00395624],\n",
       "       ...,\n",
       "       [-0.007145  ,  0.00733935,  0.00203389, ...,  0.00828057,\n",
       "        -0.00762024,  0.00704344],\n",
       "       [ 0.00082475,  0.00584078,  0.00168824, ...,  0.00324385,\n",
       "        -0.0008019 ,  0.00289006],\n",
       "       [ 0.00197069,  0.00593258, -0.00142932, ...,  0.00097622,\n",
       "        -0.00181871,  0.00134782]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "qrvdN7WSBmzC"
   },
   "outputs": [],
   "source": [
    "sample_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aajkRYaBInCA",
    "outputId": "a8388262-6e8b-422a-a21b-b32ac936c989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([180, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RNBwvlU4Hsgx"
   },
   "outputs": [],
   "source": [
    "sample_indices = tf.squeeze(sample_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jh9Xhq1CIACX"
   },
   "outputs": [],
   "source": [
    "#ind_to_char[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cJbiq7VEIsVf"
   },
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MnxtqXgJbHZ",
    "outputId": "f562e700-d0cc-452a-dead-14a3ea13d59d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 91s 365ms/step - loss: 2.7594\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 91s 377ms/step - loss: 1.9362\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 91s 378ms/step - loss: 1.6318\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 91s 376ms/step - loss: 1.4484\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.3467\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 91s 376ms/step - loss: 1.2853\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.2431\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 91s 376ms/step - loss: 1.2113\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.1860\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 90s 374ms/step - loss: 1.1649\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.1465\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 91s 377ms/step - loss: 1.1300\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.1151\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 91s 378ms/step - loss: 1.1012\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.0887\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 90s 374ms/step - loss: 1.0765\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.0643\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 91s 376ms/step - loss: 1.0526\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 91s 374ms/step - loss: 1.0418\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 91s 375ms/step - loss: 1.0307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d956abe50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0nqOK53JxXQ"
   },
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "MfEUgjY1KIRn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "weCwUieEMeRz"
   },
   "outputs": [],
   "source": [
    "model.save('shakespeare_gen.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "zFRFKoZAKa-e"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size,embd_dim,rnn_neurons,batch_size=1)\n",
    "\n",
    "model.load_weights('shakespeare_gen.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXDMzkTnLcZH",
    "outputId": "ceb079be-cf0c-4301-9dcf-ef32aff7a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ijWyJnd3lyWF"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed,gen_size=500,temp=1.0):\n",
    "\n",
    "  num_generate = gen_size\n",
    "\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  tempature = temp\n",
    "\n",
    "# https://stackoverflow.com/questions/42763928/how-to-use-model-reset-states-in-keras\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions,0)\n",
    "    predictions = predictions/tempature\n",
    "\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id],0)\n",
    "\n",
    "    text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed+\"\".join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwGsThdDuLMC",
    "outputId": "47c89fa8-8181-4e49-cc25-dd962b12e016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALEXDDY. Well, my good lord- Indeed, I will not take a common time;\n",
      "    Therefore, two ships.\n",
      "  YORK. Whe posts the golden death of winters follow thoughs,\n",
      "    As fell to another weary a many of his face\n",
      "      After his lance for 't. Now I am commanded\n",
      "  Come on to seek the cap; of leading loves they die.\n",
      "  The substake their affairs, and from our fight  \n",
      "    And slay thine own mouth. Yea, not a foolish homicius,\n",
      "    Offering none of mickleman 'nhappele,\n",
      "    And catch it by. Be it confounded then?\n",
      "    Urle sleeping! yes, my lord, within your dowrul,\n",
      "    And his own birds.                             [Advancing]  I know your master's brand,\n",
      "    Nor fair mistress be employ'd withal\n",
      "    As water-loss about the guard of grace;\n",
      "    So went illy Lucius, I found. Ay, sir, is 'ald will keep other off\n",
      "    Satisfyray it by callett Prospero.\n",
      "  ROSS. Be it to me. Mine ear I keep your bosom  \n",
      "    As hoon did gather, by your free and officer or him must needs be blunt.\n",
      "  Lear. Romeo, present; pardon me;\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,'ALEX',gen_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfqmD9-Qy1-f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
